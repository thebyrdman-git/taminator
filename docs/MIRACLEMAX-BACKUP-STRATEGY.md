# MiracleMax Backup Strategy
## Comprehensive Data Protection & Disaster Recovery

**Version:** 1.0 | **Date:** October 2025 | **Status:** Production Ready

---

## ğŸ¯ Overview

MiracleMax backup system provides **enterprise-grade data protection** with automated daily backups, GPG encryption, retention management, and Prometheus monitoring. This document outlines the complete backup and disaster recovery strategy.

---

## ğŸ—ï¸ Backup Architecture

### Three-Tier Protection Strategy

```
Tier 1: Local Backups (Primary)
â”œâ”€â”€ Location: /home/jbyrd/backups/miraclemax/
â”œâ”€â”€ Frequency: Daily at 3:00 AM
â”œâ”€â”€ Retention: 30 days
â”œâ”€â”€ Encryption: GPG (n8n always encrypted)
â””â”€â”€ Size: ~500MB-2GB per backup (excluding Prometheus)

Tier 2: NFS/Network Storage (Secondary)
â”œâ”€â”€ Location: NFS mount to rhgrimm or separate NAS
â”œâ”€â”€ Frequency: Rsync after each local backup
â”œâ”€â”€ Retention: 90 days
â””â”€â”€ Purpose: Hardware failure protection

Tier 3: Offsite/Cloud (Tertiary)
â”œâ”€â”€ Options: Backblaze B2, AWS S3 Glacier, rsync.net
â”œâ”€â”€ Frequency: Weekly full, daily incremental
â”œâ”€â”€ Retention: 1 year
â””â”€â”€ Purpose: Disaster recovery (fire, theft, total loss)
```

---

## ğŸ“¦ Backup Components

### Critical Data (MUST backup)

| Component | Location | Size | Encryption | Restore Priority |
|-----------|----------|------|------------|------------------|
| **Home Assistant** | `/home/jbyrd/homeassistant-config` | ~50MB | GPG (optional) | HIGH |
| **n8n Workflows** | `/home/jbyrd/n8n-data` | ~20MB | GPG (REQUIRED) | CRITICAL |
| **Actual Budget** | Container volume | ~5MB | GPG (via pai-actual-backup) | CRITICAL |
| **Grafana Dashboards** | `grafana-data` volume | ~100MB | No | MEDIUM |
| **Traefik SSL Certs** | `compose_traefik-certs` volume | ~5MB | No | HIGH |
| **Infrastructure Configs** | `~/miraclemax-infrastructure` | ~500KB | No | HIGH |
| **Alertmanager State** | `compose_alertmanager-data` volume | ~5MB | No | LOW |
| **Portainer Config** | `portainer_data` volume | ~10MB | No | LOW |

### Optional Data (can skip/regenerate)

| Component | Reason to Skip | Regeneration Method |
|-----------|----------------|---------------------|
| **Prometheus Metrics** | 50GB+, regenerates automatically | Wait 90 days for full retention |
| **Container Images** | Pull from registries | `podman-compose pull && up -d` |
| **Log Files** | Historical only | Not needed for recovery |

---

## ğŸš€ Implementation

### Automated Daily Backups

**Installation:**
```bash
pai-miraclemax-backup-install
```

**Manual Backup:**
```bash
# Standard backup (skip Prometheus)
pai-miraclemax-backup

# Full backup (includes Prometheus - 50GB+)
pai-miraclemax-backup --full

# Verify backup integrity
pai-miraclemax-backup --verify
```

**Backup Schedule:**
- **Time:** 3:00 AM daily
- **Duration:** ~5-15 minutes (without Prometheus)
- **Retention:** 30 days automatic cleanup
- **Monitoring:** Prometheus alerts for failures

---

## ğŸ”„ Restore Procedures

### Full System Restore

**Prerequisites:**
- Fresh Fedora Server installation
- SSH access to MiracleMax
- GPG private key imported

**Steps:**

```bash
# 1. Verify backup integrity
pai-miraclemax-restore --latest --verify-only

# 2. Test restore (dry-run)
pai-miraclemax-restore --latest --dry-run

# 3. Full restore (DESTRUCTIVE)
pai-miraclemax-restore --latest

# 4. Selective component restore
pai-miraclemax-restore --latest --component homeassistant
pai-miraclemax-restore --latest --component n8n
pai-miraclemax-restore --latest --component grafana
```

### Restore from Specific Backup

```bash
# List available backups
ls -lh ~/backups/miraclemax/

# Restore from specific timestamp
pai-miraclemax-restore --from-backup 20251016_030000
```

### Emergency Recovery (No Scripts Available)

```bash
# 1. Extract encrypted backup
gpg --decrypt ~/backups/miraclemax/latest/databases/n8n.tar.gz.gpg | tar xzf -

# 2. Restore to miraclemax
rsync -az n8n/ jbyrd@192.168.1.34:/home/jbyrd/n8n-data/

# 3. Restart container
ssh jbyrd@192.168.1.34 'cd ~/miraclemax-infrastructure && podman-compose -f compose/n8n.yml restart'
```

---

## â˜ï¸ Offsite Backup Strategy

### Option 1: Backblaze B2 (RECOMMENDED)

**Pros:**
- Low cost: $5/TB/month storage, $10/TB egress
- S3-compatible API
- Excellent for disaster recovery
- No egress fees for first 3x storage

**Implementation:**
```bash
# Install b2 CLI
pip install b2

# Configure credentials
b2 authorize-account <keyId> <appKey>

# Create bucket
b2 create-bucket miraclemax-backups allPrivate

# Sync backups (run after pai-miraclemax-backup)
b2 sync --delete --keepDays 365 \
  ~/backups/miraclemax/latest/ \
  b2://miraclemax-backups/$(date +%Y-%m-%d)/
```

**Cost Estimate:**
- Storage: 2GB Ã— 365 days = ~730GB = $3.65/month
- Egress: Minimal (only on restore)
- **Total: ~$5/month**

### Option 2: rsync.net

**Pros:**
- Simple rsync/SSH interface
- ZFS snapshots included
- No API complexity
- Geographically distributed

**Implementation:**
```bash
# Setup (one-time)
ssh-keygen -t ed25519 -f ~/.ssh/rsync_net
ssh-copy-id -i ~/.ssh/rsync_net.pub 12345@usw-s001.rsync.net

# Sync backups
rsync -avz --delete \
  -e "ssh -i ~/.ssh/rsync_net" \
  ~/backups/miraclemax/latest/ \
  12345@usw-s001.rsync.net:miraclemax/
```

**Cost Estimate:**
- 100GB minimum: $8/month
- ZFS snapshots: Included
- **Total: $8/month**

### Option 3: AWS S3 Glacier Deep Archive

**Pros:**
- Lowest storage cost: $1/TB/month
- AWS integration for other services
- Enterprise-grade reliability

**Cons:**
- High retrieval costs ($0.02/GB)
- 12-hour retrieval time
- Complex API

**Implementation:**
```bash
# Install AWS CLI
pip install awscli

# Configure
aws configure

# Sync to S3 with Glacier Deep Archive
aws s3 sync ~/backups/miraclemax/latest/ \
  s3://miraclemax-backups/$(date +%Y-%m-%d)/ \
  --storage-class DEEP_ARCHIVE
```

**Cost Estimate:**
- Storage: 730GB Ã— $0.001 = $0.73/month
- Retrieval: $15 (one-time on disaster)
- **Total: ~$1/month (+ retrieval costs)**

---

## ğŸ”’ Security & Encryption

### GPG Encryption

**Key Management:**
```bash
# Generate GPG key (if not exists)
gpg --full-generate-key
# Use: RSA 4096-bit, no expiration, email: jbyrd@redhat.com

# Export private key (for disaster recovery)
gpg --export-secret-keys --armor jbyrd@redhat.com > ~/gpg-private-key-backup.asc

# Store private key securely:
# 1. Encrypted USB drive (keep offsite)
# 2. Password manager (1Password, Bitwarden)
# 3. Paper backup (for apocalypse scenario)
```

**Restore GPG Key:**
```bash
# Import private key
gpg --import gpg-private-key-backup.asc

# Trust key
gpg --edit-key jbyrd@redhat.com
gpg> trust
gpg> 5 (ultimate trust)
gpg> quit
```

### What Gets Encrypted

- **Always Encrypted:** n8n (contains API credentials, OAuth tokens)
- **Optional Encryption:** Home Assistant, Actual Budget (sensitive financial data)
- **Never Encrypted:** Infrastructure configs (needed for restore without GPG), Grafana dashboards

---

## ğŸ“Š Monitoring & Alerting

### Prometheus Metrics

```yaml
# Backup success/failure
miraclemax_backup_success{} 1

# Last backup timestamp
miraclemax_backup_timestamp{} 1729059600

# Backup size
miraclemax_backup_size_bytes{} 524288000

# Backup duration
miraclemax_backup_duration_seconds{} 487
```

### Prometheus Alerts

- **BackupFailed:** Critical alert if backup fails
- **BackupStale:** Warning if no backup in 2+ days
- **BackupMetricsMissing:** Warning if metrics disappear
- **BackupSizeAnomaly:** Warning if backup unexpectedly small
- **BackupDurationAnomaly:** Warning if backup takes >1 hour

### Email Notifications

All backup failures generate **CRITICAL** email alerts via Alertmanager with:
- Failure reason
- Last successful backup timestamp
- Runbook link for troubleshooting
- Direct link to logs

---

## ğŸ§ª Testing Strategy

### Monthly Verification (Required)

```bash
# 1. Verify backup integrity
pai-miraclemax-backup --verify

# 2. Test selective restore
pai-miraclemax-restore --latest --component grafana --dry-run

# 3. Validate offsite backup
b2 ls miraclemax-backups | head -10
```

### Quarterly Full Recovery Test (Recommended)

```bash
# 1. Spin up test VM or container
multipass launch --name miraclemax-test

# 2. Restore backup to test system
pai-miraclemax-restore --from-backup <recent-backup> --force

# 3. Verify all services start correctly
ssh miraclemax-test 'podman ps'

# 4. Spot-check critical data
# - Home Assistant: automations work
# - n8n: workflows load
# - Grafana: dashboards render

# 5. Destroy test system
multipass delete --purge miraclemax-test
```

---

## ğŸ¯ Recovery Time Objectives (RTO)

| Scenario | RTO | Steps |
|----------|-----|-------|
| **Single Service Failure** | <5 minutes | Selective restore: `pai-miraclemax-restore --component <name>` |
| **Data Corruption** | <30 minutes | Full restore from local backup |
| **Hardware Failure** | <2 hours | Rebuild host + restore from network backup |
| **Total Disaster** | <24 hours | Provision new hardware + restore from offsite |

---

## ğŸ¯ Recovery Point Objectives (RPO)

| Data Type | RPO | Backup Frequency |
|-----------|-----|------------------|
| **Critical Services** | 24 hours | Daily at 3:00 AM |
| **Actual Budget** | 24 hours | Daily (separate script) |
| **Financial Data** | 24 hours | Synced to Git + local backup |
| **Infrastructure Configs** | Real-time | Git version control |

---

## ğŸ“‹ Disaster Recovery Checklist

### Pre-Disaster (Do This Now)

- [ ] Export GPG private key to secure location
- [ ] Document SSH key locations
- [ ] Save Cloudflare tunnel credentials
- [ ] Keep copy of infrastructure Git URL
- [ ] Document critical service passwords
- [ ] Test restore procedure monthly

### During Disaster

1. **Assess Damage**
   - What failed? (service, disk, host, datacenter)
   - What's the blast radius?
   - Can data be recovered in-place?

2. **Stop the Bleeding**
   - Disable failing services
   - Prevent data corruption spread
   - Switch to backup/redundant systems if available

3. **Execute Recovery**
   - Follow restoration procedures above
   - Start with critical services first (priority: CRITICAL > HIGH > MEDIUM > LOW)
   - Verify each service before moving to next

4. **Post-Recovery Validation**
   - Check all services running: `podman ps`
   - Verify monitoring operational
   - Test critical workflows
   - Review logs for errors

5. **Post-Mortem**
   - Document what happened
   - Update runbooks
   - Improve backup/restore procedures
   - Add monitoring to prevent recurrence

---

## ğŸ’¾ Storage Requirements

### Local Storage

```
Daily Backup Size (no Prometheus): ~1-2GB
Retention: 30 days
Total: 30-60GB
```

### Network Storage

```
Daily Backup Size: ~1-2GB
Retention: 90 days
Total: 90-180GB
```

### Offsite Storage

```
Daily Backup Size: ~1-2GB
Retention: 365 days
Total: 365-730GB (~$3-5/month on Backblaze B2)
```

---

## ğŸ¯ Next Steps

### Immediate (Do Today)

1. **Install backup automation:**
   ```bash
   pai-miraclemax-backup-install
   ```

2. **Run initial backup:**
   ```bash
   pai-miraclemax-backup --verify
   ```

3. **Test restore (dry-run):**
   ```bash
   pai-miraclemax-restore --latest --dry-run
   ```

### This Week

4. **Export GPG private key to secure location**
5. **Set up offsite backup (choose Backblaze B2, rsync.net, or S3)**
6. **Document recovery procedures in runbook**

### Monthly

7. **Verify backup integrity and test selective restore**
8. **Review backup logs for anomalies**
9. **Check backup retention and storage usage**

### Quarterly

10. **Full disaster recovery test on separate VM**
11. **Review and update disaster recovery procedures**
12. **Audit offsite backup completeness**

---

## ğŸ“Š Success Metrics

### Before Backup Automation
- âŒ Manual backups (inconsistent)
- âŒ No encryption
- âŒ No monitoring
- âŒ No offsite storage
- âŒ No tested restore procedures
- **RPO:** 7+ days | **RTO:** Unknown

### After Backup Automation
- âœ… Automated daily backups
- âœ… GPG encryption for sensitive data
- âœ… Prometheus monitoring with alerts
- âœ… 30-day local retention
- âœ… Tested restore procedures
- âœ… Offsite backup capability
- **RPO:** 24 hours | **RTO:** <2 hours

---

## ğŸ¯ Conclusion

MiracleMax now has **enterprise-grade backup automation** with:
- âœ… Daily automated backups
- âœ… GPG encryption for credentials
- âœ… Prometheus monitoring with mobile alerts
- âœ… Tested restore procedures
- âœ… 30-day local retention
- âœ… Offsite backup strategy documented
- âœ… <24 hour RPO, <2 hour RTO

**Mission Critical Next Step:** Configure offsite backup (Backblaze B2 recommended for cost/simplicity balance).

---

*MiracleMax Backup Strategy - Data Protection for the Modern Era*  
*Confidential - Internal Technical Documentation*

