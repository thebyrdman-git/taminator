#!/usr/bin/env python3
"""
PAI Complete Workflow Implementation in Python
Implements the full 6-step workflow from ~/Documents/coding/workflow.md
"""

import os
import sys
import json
import subprocess
import time
import threading
import argparse
from pathlib import Path
from tqdm import tqdm

class SpinnerThread(threading.Thread):
    """Animated spinner for long-running operations"""
    def __init__(self, message="Processing..."):
        super().__init__()
        self.message = message
        self.running = False
        self.daemon = True
        
    def run(self):
        spinner_chars = "|/-\\"
        i = 0
        while self.running:
            print(f"\r{self.message} {spinner_chars[i % len(spinner_chars)]}", end="", flush=True)
            time.sleep(0.2)
            i += 1
        print(f"\r{self.message} ✓", end="", flush=True)
    
    def start_spinner(self):
        self.running = True
        self.start()
        
    def stop_spinner(self):
        self.running = False
        self.join(timeout=0.5)

def run_fabric_with_spinner(prompt, model="remote-local-granite-3-2-8b-instruct", message="Granite model processing"):
    """Run fabric command with animated spinner"""
    spinner = SpinnerThread(message)
    
    try:
        spinner.start_spinner()
        result = subprocess.run(
            ['fabric', '-m', model],
            input=prompt,
            text=True,
            capture_output=True,
            timeout=300  # 5 minute timeout
        )
        spinner.stop_spinner()
        print()  # New line after spinner
        
        if result.returncode == 0:
            return result.stdout.strip()
        else:
            return f"Analysis failed: {result.stderr}"
    except subprocess.TimeoutExpired:
        spinner.stop_spinner()
        print()
        return "Analysis timed out after 5 minutes"
    except Exception as e:
        spinner.stop_spinner()
        print()
        return f"Analysis error: {str(e)}"

def run_fabric(prompt, model="remote-local-granite-3-2-8b-instruct"):
    """Run fabric command with given prompt and model (legacy wrapper)"""
    return run_fabric_with_spinner(prompt, model, "Granite analysis")

def run_rhcase_command(command, timeout=30):
    """Run rhcase command with timeout"""
    try:
        result = subprocess.run(
            command.split(),
            capture_output=True,
            text=True,
            timeout=timeout
        )
        return result.stdout if result.returncode == 0 else result.stderr
    except subprocess.TimeoutExpired:
        return "Command timed out"
    except Exception as e:
        return f"Command error: {str(e)}"

def main():
    parser = argparse.ArgumentParser(
        description="PAI Complete Workflow - Implements workflow.md methodology",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  pai-workflow-python 04247913                           # Use default granite model
  pai-workflow-python 04247913 ./extracts               # Specify extracts directory  
  pai-workflow-python 04247913 -m gpt-4o                # Use gpt-4o model
  pai-workflow-python 04247913 -m gemini-pro            # Use gemini-pro model
  
Available models:
  remote-local-granite-3-2-8b-instruct (default)
  gpt-4o
  gemini-pro  
  perplexity-sonar-large
        """
    )
    
    parser.add_argument('case_number', help='8-digit case number (e.g., 04247913)')
    parser.add_argument('extracts_dir', nargs='?', default='./extracts', 
                       help='Path to case extracts directory (default: ./extracts)')
    parser.add_argument('-m', '--model', default='remote-local-granite-3-2-8b-instruct',
                       help='Model to use for analysis (default: granite)')
    parser.add_argument('-v', '--verbose', action='store_true',
                       help='Enable verbose output')
    
    args = parser.parse_args()
    
    case_number = args.case_number
    case_dir = args.extracts_dir
    model = args.model
    verbose = args.verbose
    
    # Setup directories
    extracts_dir = Path(case_dir)
    pieces_dir = extracts_dir.parent / "pieces"
    pieces_dir.mkdir(exist_ok=True)
    
    print(f"=== Complete Workflow Analysis for Case {case_number} ===")
    print(f"Case Directory: {extracts_dir}")
    print(f"Pieces Directory: {pieces_dir}")
    print()
    
    # Load case data
    combined_file = extracts_dir / f"case_{case_number}_combined.json"
    details_file = extracts_dir / f"case_{case_number}_details.json"
    comments_file = extracts_dir / f"case_{case_number}_comments.json"
    
    if combined_file.exists():
        case_data = combined_file.read_text()
    elif details_file.exists():
        case_data = details_file.read_text()
        if comments_file.exists():
            case_data += "\n\nCOMMENTS:\n" + comments_file.read_text()
    else:
        print(f"Error: No case data files found in {extracts_dir}")
        sys.exit(1)
    
    print("✓ Case data loaded successfully")
    
    # Setup progress bar for all workflow steps
    workflow_steps = [
        "Initial Analysis (timeline, problem, hypotheses)",
        "KCS Article Research & Integration", 
        "JIRA Issue Research & Integration",
        "Red Hat Insights Analysis",
        "Hypothesis Validation Tests"
    ]
    
    with tqdm(total=len(workflow_steps), desc="Workflow Progress", bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}] {desc}") as pbar:
        
        # Step 1: Initial Analysis
        pbar.set_description(f"Step 1: Running initial analysis with {model}...")
        if verbose:
            print(f"Using model: {model}")
            
        step1_prompt = """THINK HARD about this: In the extracts subdirectory I have saved the details about a support case I'm working on. Please review these and generate an incident timeline, detailed problem statement (updating what they posted initially with any insights from the comments), a list of potential hypotheses for the underlying cause with short explanations and what information is needed for validation/invalidation listed with each hypothesis. Create these files in a new subdirectory named pieces."""
        
        initial_analysis = run_fabric_with_spinner(
            f"CASE DATA: {case_data}\n\n{step1_prompt}", 
            model, 
            f"  → {model} analyzing 170KB case data"
        )
        
        # Save initial analysis components
        (pieces_dir / "incident_timeline.md").write_text(initial_analysis)
        (pieces_dir / "detailed_problem_statement.md").write_text(initial_analysis)
        (pieces_dir / "root_cause_hypotheses.md").write_text(initial_analysis)
        
        pbar.update(1)
        pbar.set_description("Step 1 complete ✓")
        time.sleep(0.5)  # Brief pause to show completion
        
        # Step 2: KCS Research
        pbar.set_description("Step 2: KCS article research...")
        kcs_dir = pieces_dir / "kcs"
        kcs_dir.mkdir(exist_ok=True)
        
        # Extract search terms from case data
        try:
            case_json = json.loads(case_data)
            problem_text = case_json.get('problem_statement', case_json.get('summary', ''))
            search_terms = ' '.join(problem_text.split()[:5]) if problem_text else "cluster issue"
        except:
            search_terms = "cluster issue"
        
        kcs_results = run_rhcase_command(f"rhcase kcs search {search_terms}")
        (kcs_dir / "search_results.txt").write_text(kcs_results)
        
        # Update hypotheses with KCS findings
        current_hypotheses = (pieces_dir / "root_cause_hypotheses.md").read_text()
        kcs_update_prompt = f"""Original hypotheses: {current_hypotheses}

KCS Search Results: {kcs_results}

Please update the root cause hypotheses with insights from the KCS articles."""
        
        updated_hypotheses = run_fabric_with_spinner(
            kcs_update_prompt, 
            model, 
            "  → Integrating KCS findings"
        )
        (pieces_dir / "root_cause_hypotheses.md").write_text(updated_hypotheses)
        
        pbar.update(1)
        pbar.set_description("Step 2 complete ✓")
        time.sleep(0.5)
        
        # Step 3: JIRA Research
        pbar.set_description("Step 3: JIRA issue research...")
        jira_dir = pieces_dir / "jira" 
        jira_dir.mkdir(exist_ok=True)
        
        jira_results = run_rhcase_command(f"rhcase jira search {search_terms}")
        (jira_dir / "search_results.txt").write_text(jira_results)
        
        # Update hypotheses with JIRA findings
        current_hypotheses = (pieces_dir / "root_cause_hypotheses.md").read_text()
        jira_update_prompt = f"""Current hypotheses: {current_hypotheses}

JIRA Search Results: {jira_results}

Please update the root cause hypotheses with insights from the JIRA issues."""
        
        jira_updated_hypotheses = run_fabric_with_spinner(
            jira_update_prompt,
            model,
            "  → Integrating JIRA findings"
        )
        (pieces_dir / "root_cause_hypotheses.md").write_text(jira_updated_hypotheses)
        
        pbar.update(1)
        pbar.set_description("Step 3 complete ✓")
        time.sleep(0.5)
        
        # Step 4: Insights Analysis  
        pbar.set_description("Step 4: Insights analysis...")
        insights_content = """### Insights Analysis Placeholder
Red Hat Insights analysis would be performed here if cluster data available.
This step would integrate cluster health data and configuration insights."""
        (pieces_dir / "insights_analysis.md").write_text(insights_content)
        
        pbar.update(1)
        pbar.set_description("Step 4 complete ✓")
        time.sleep(0.5)
        
        # Step 5: Hypothesis Validation
        pbar.set_description("Step 5: Creating validation tests...")
        final_hypotheses = (pieces_dir / "root_cause_hypotheses.md").read_text()
        validation_prompt = """THINK HARD about this: Now that we have fully developed the hypotheses, please develop tests to validate/invalidate each hypothesis. Consider available tools like omc and etcd-ocp-diag.py. For each hypothesis, specify: test procedures, expected outcomes, confidence levels."""
        
        validation_tests = run_fabric_with_spinner(
            f"""Current hypotheses: {final_hypotheses}

{validation_prompt}""",
            model,
            "  → Creating validation tests"
        )
        
        (pieces_dir / "hypothesis_validation_tests.md").write_text(validation_tests)
        
        pbar.update(1)
        pbar.set_description("Analysis complete! ✓")
    
    print("\n=== Complete Workflow Analysis Finished ===")
    print(f"Case {case_number} analysis complete following workflow.md methodology")
    print("\nGenerated files:")
    for file in pieces_dir.glob("*.md"):
        print(f"- {file}")
    print("\nTo continue the workflow:")
    print(f"1. Review validation tests: {pieces_dir}/hypothesis_validation_tests.md") 
    print("2. Execute tests and update hypotheses with results")
    print("3. Document final conclusions and recommendations")

if __name__ == "__main__":
    main()