groups:
  - name: miraclemax_host
    interval: 30s
    rules:
      # CPU Alerts
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 75
        for: 5m
        labels:
          severity: warning
          component: host
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanize }}% (threshold: 75%)"
          runbook: "Check top processes: ssh miraclemax 'top -b -n 1 | head -20'"
      
      - alert: CriticalCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 90
        for: 2m
        labels:
          severity: critical
          component: host
        annotations:
          summary: "CRITICAL: CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanize }}% (threshold: 90%)"
          runbook: "Immediate action required. Check processes and consider restarting services."
      
      # Memory Alerts
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 80
        for: 5m
        labels:
          severity: warning
          component: host
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanize }}% (threshold: 80%)"
          runbook: "Check memory-hungry containers: podman stats --no-stream"
      
      - alert: CriticalMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
        for: 2m
        labels:
          severity: critical
          component: host
        annotations:
          summary: "CRITICAL: Memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanize }}% (threshold: 90%)"
          runbook: "OOM risk. Consider restarting non-critical containers immediately."
      
      # Disk Alerts
      - alert: HighDiskUsage
        expr: (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.*"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.*"})) * 100 > 85
        for: 5m
        labels:
          severity: warning
          component: host
        annotations:
          summary: "High disk usage on {{ $labels.instance }}:{{ $labels.mountpoint }}"
          description: "Disk usage is {{ $value | humanize }}% (threshold: 85%)"
          runbook: "Run cleanup: docker system prune -af; check logs in /var/log"
      
      - alert: CriticalDiskUsage
        expr: (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.*"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.*"})) * 100 > 95
        for: 2m
        labels:
          severity: critical
          component: host
        annotations:
          summary: "CRITICAL: Disk usage on {{ $labels.instance }}:{{ $labels.mountpoint }}"
          description: "Disk usage is {{ $value | humanize }}% (threshold: 95%)"
          runbook: "Emergency cleanup required. Service failures imminent."
      
      # Tmpfs Alerts (separate monitoring for /tmp and other tmpfs mounts)
      - alert: HighTmpfsUsage
        expr: (1 - (node_filesystem_avail_bytes{fstype="tmpfs"} / node_filesystem_size_bytes{fstype="tmpfs"})) * 100 > 80
        for: 3m
        labels:
          severity: warning
          component: host
        annotations:
          summary: "High tmpfs usage on {{ $labels.instance }}:{{ $labels.mountpoint }}"
          description: "Tmpfs usage is {{ $value | humanize }}% (threshold: 80%)"
          runbook: "Clean temp files: find {{ $labels.mountpoint }} -type f -atime +7 -delete"
      
      - alert: CriticalTmpfsUsage
        expr: (1 - (node_filesystem_avail_bytes{fstype="tmpfs"} / node_filesystem_size_bytes{fstype="tmpfs"})) * 100 > 90
        for: 1m
        labels:
          severity: critical
          component: host
        annotations:
          summary: "CRITICAL: Tmpfs usage on {{ $labels.instance }}:{{ $labels.mountpoint }}"
          description: "Tmpfs usage is {{ $value | humanize }}% (threshold: 90%)"
          runbook: "Emergency cleanup required. Check largest files: du -sh {{ $labels.mountpoint }}/* | sort -h"
      
      # Network Alerts
      - alert: HighNetworkErrors
        expr: rate(node_network_receive_errs_total[5m]) + rate(node_network_transmit_errs_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
          component: network
        annotations:
          summary: "High network errors on {{ $labels.instance }}"
          description: "Network error rate: {{ $value | humanize }} errors/sec"
          runbook: "Check network interface health: ip link show; dmesg | grep eth"
      
      # Host Down
      - alert: HostDown
        expr: up{job="node-exporter"} == 0
        for: 1m
        labels:
          severity: critical
          component: host
        annotations:
          summary: "Host {{ $labels.instance }} is down"
          description: "Node exporter has been unreachable for 1 minute"
          runbook: "Check host connectivity. Physical access may be required."

  - name: miraclemax_containers
    interval: 30s
    rules:
      # Container Down
      - alert: ContainerDown
        expr: up{job="cadvisor"} == 0
        for: 2m
        labels:
          severity: critical
          component: container
        annotations:
          summary: "Container monitoring down"
          description: "cAdvisor has been unreachable for 2 minutes"
          runbook: "Check cAdvisor container: podman ps -a | grep cadvisor"
      
      # Container Restarts
      - alert: FrequentContainerRestarts
        expr: rate(container_last_seen{name!=""}[10m]) > 2
        for: 5m
        labels:
          severity: warning
          component: container
        annotations:
          summary: "Container {{ $labels.name }} restarting frequently"
          description: "Container has restarted {{ $value | humanize }} times in 10 minutes"
          runbook: "Check logs: podman logs {{ $labels.name }}; check OOM or crash"
      
      # Container CPU
      - alert: ContainerHighCPU
        expr: rate(container_cpu_usage_seconds_total{name!=""}[5m]) > 0.8
        for: 5m
        labels:
          severity: warning
          component: container
        annotations:
          summary: "Container {{ $labels.name }} high CPU usage"
          description: "Container CPU usage: {{ $value | humanize }}"
          runbook: "Check container resource limits and application performance"
      
      # Container Memory
      - alert: ContainerHighMemory
        expr: (container_memory_usage_bytes{name!=""} / container_spec_memory_limit_bytes{name!=""}) > 0.9
        for: 5m
        labels:
          severity: warning
          component: container
        annotations:
          summary: "Container {{ $labels.name }} high memory usage"
          description: "Container memory usage: {{ $value | humanizePercentage }}"
          runbook: "Check for memory leaks or increase container limits"

  - name: miraclemax_services
    interval: 30s
    rules:
      # Traefik Down
      - alert: TraefikDown
        expr: absent(up{job="traefik"})
        for: 1m
        labels:
          severity: critical
          component: service
          service: traefik
        annotations:
          summary: "Traefik reverse proxy is down"
          description: "All external services are unreachable"
          runbook: "Restart Traefik immediately: podman restart traefik"
      
      # Authelia Down
      - alert: AutheliaDown
        expr: absent(up{job="authelia"})
        for: 2m
        labels:
          severity: critical
          component: service
          service: authelia
        annotations:
          summary: "Authelia MFA service is down"
          description: "Authentication unavailable for all services"
          runbook: "Restart Authelia: podman restart authelia; check logs"
      
      # Prometheus Down (self-monitoring)
      - alert: PrometheusDown
        expr: absent(up{job="prometheus"})
        for: 1m
        labels:
          severity: critical
          component: monitoring
        annotations:
          summary: "Prometheus is down"
          description: "Monitoring and alerting system is offline"
          runbook: "Restart Prometheus immediately"

  - name: miraclemax_backups
    interval: 60s
    rules:
      # Backup Failure
      - alert: BackupFailed
        expr: miraclemax_backup_success == 0
        for: 5m
        labels:
          severity: critical
          component: backup
        annotations:
          summary: "MiracleMax backup failed"
          description: "Last backup attempt failed. Data protection compromised."
          runbook: "Check logs: journalctl -u pai-miraclemax-backup.service -n 50"
      
      # Backup Age (stale backups)
      - alert: BackupStale
        expr: (time() - miraclemax_backup_timestamp) > 86400 * 2
        for: 1h
        labels:
          severity: warning
          component: backup
        annotations:
          summary: "MiracleMax backup is stale"
          description: "Last successful backup was {{ $value | humanizeDuration }} ago (threshold: 2 days)"
          runbook: "Check backup timer: systemctl status pai-miraclemax-backup.timer; run manual backup: pai-miraclemax-backup"
      
      # Backup Missing (no backup metrics)
      - alert: BackupMetricsMissing
        expr: absent(miraclemax_backup_timestamp)
        for: 2h
        labels:
          severity: warning
          component: backup
        annotations:
          summary: "MiracleMax backup metrics missing"
          description: "No backup metrics found. Backup system may not be running."
          runbook: "Verify backup timer: systemctl list-timers pai-miraclemax-backup.timer"
      
      # Backup Size Anomaly (backup too small - might indicate incomplete backup)
      - alert: BackupSizeAnomaly
        expr: miraclemax_backup_size_bytes < 100000000
        for: 5m
        labels:
          severity: warning
          component: backup
        annotations:
          summary: "MiracleMax backup size anomaly"
          description: "Backup size is {{ $value | humanize1024 }}B (expected >100MB). May be incomplete."
          runbook: "Verify backup contents: ls -lh /home/jbyrd/backups/miraclemax/latest/"
      
      # Backup Duration Anomaly (taking too long)
      - alert: BackupDurationAnomaly
        expr: miraclemax_backup_duration_seconds > 3600
        for: 5m
        labels:
          severity: warning
          component: backup
        annotations:
          summary: "MiracleMax backup taking too long"
          description: "Backup took {{ $value | humanizeDuration }} (threshold: 1 hour). Performance issue detected."
          runbook: "Check network connectivity to miraclemax; check disk I/O: iostat -x 1 5"

  # Log Aggregation Group (Loki + Promtail)
  - name: miraclemax_logs
    interval: 60s
    rules:
      # Loki Service Down
      - alert: LokiDown
        expr: up{job="loki"} == 0
        for: 2m
        labels:
          severity: critical
          component: logging
        annotations:
          summary: "Loki log aggregation service is down"
          description: "Loki has been down for more than 2 minutes. Log collection is stopped."
          runbook: "Check Loki status: ssh miraclemax 'podman logs loki --tail 50'"
      
      # Promtail Service Down
      - alert: PromtailDown
        expr: up{job="promtail"} == 0
        for: 2m
        labels:
          severity: critical
          component: logging
        annotations:
          summary: "Promtail log collection agent is down"
          description: "Promtail has been down for more than 2 minutes. Logs are not being collected."
          runbook: "Check Promtail status: ssh miraclemax 'podman logs promtail --tail 50'"
      
      # Loki Storage Space Warning
      - alert: LokiHighDiskUsage
        expr: (loki_storage_bucket_files_total / 100000) > 0.8
        for: 10m
        labels:
          severity: warning
          component: logging
        annotations:
          summary: "Loki disk usage approaching limit"
          description: "Loki storage is {{ $value | humanizePercentage }} full. Consider increasing retention or storage."
          runbook: "Check Loki storage: ssh miraclemax 'du -sh /var/lib/containers/storage/volumes/loki-data'"
      
      # High Log Ingestion Rate
      - alert: HighLogIngestionRate
        expr: rate(loki_distributor_bytes_received_total[5m]) > 10485760
        for: 10m
        labels:
          severity: warning
          component: logging
        annotations:
          summary: "Unusually high log ingestion rate"
          description: "Loki is ingesting {{ $value | humanize1024 }}B/s of logs (threshold: 10MB/s). Check for log spam."
          runbook: "Check which containers are logging heavily via Grafana Explore"
      
      # Promtail Read Errors
      - alert: PromtailReadErrors
        expr: rate(promtail_read_errors_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          component: logging
        annotations:
          summary: "Promtail experiencing read errors"
          description: "Promtail has {{ $value }} read errors per second. Check container log permissions."
          runbook: "Check Promtail logs: ssh miraclemax 'podman logs promtail --tail 100 | grep -i error'"
      
      # Loki Query Performance
      - alert: SlowLogQueries
        expr: histogram_quantile(0.99, rate(loki_request_duration_seconds_bucket{route="loki_api_v1_query_range"}[5m])) > 30
        for: 10m
        labels:
          severity: warning
          component: logging
        annotations:
          summary: "Loki queries are slow"
          description: "99th percentile query latency is {{ $value | humanizeDuration }}. Performance degraded."
          runbook: "Check Loki resource usage and query patterns in Grafana"
      
      # No Logs Being Received
      - alert: NoLogsReceived
        expr: rate(loki_distributor_bytes_received_total[10m]) == 0
        for: 10m
        labels:
          severity: warning
          component: logging
        annotations:
          summary: "Loki is not receiving any logs"
          description: "No logs have been received for 10 minutes. Check Promtail and container logs."
          runbook: "Verify Promtail is running and containers are logging: ssh miraclemax 'podman ps'"

