# Taminator Testing Strategy

**Approach:** Hybrid testing combining real user testing and automated simulations

---

## ğŸ¯ Testing Philosophy

Taminator uses a **hybrid testing approach** that balances:
- **Real testing** by the user (Jimmy) for UX-critical features
- **Simulated testing** for automation, unit tests, and repeatable checks

---

## ğŸ‘¤ Real Testing (User-Driven)

### What Requires Real Testing

**1. GUI Functionality**
- âœ… Window opens and renders correctly
- âœ… Navigation between views
- âœ… Button clicks and interactions
- âœ… Forms and input fields
- âœ… Real-time updates (auth status)
- âœ… Visual design and branding
- âœ… Cross-platform behavior (Linux, macOS, Windows)

**Test Method:** Launch GUI (`npm start`), manually interact, verify visually

**2. Authentication Workflows**
- âœ… VPN connection detection
- âœ… Kerberos ticket validation
- âœ… Token configuration wizard
- âœ… Pre-flight checks blocking commands
- âœ… Error messages and guidance

**Test Method:** Run commands without auth, configure auth, verify blocking/passing

**3. End-to-End Workflows**
- âœ… Complete customer onboarding flow
- âœ… Check â†’ Update â†’ Post sequence
- âœ… Issue reporting to GitHub
- âœ… Report generation and backup

**Test Method:** Walk through complete user journey, verify each step

**4. UX and Error Messages**
- âœ… Error messages are clear and helpful
- âœ… Progress indicators show during operations
- âœ… Confirmation prompts work as expected
- âœ… Color coding and formatting is readable

**Test Method:** Trigger errors intentionally, verify messages

---

## ğŸ¤– Simulated Testing (Automated)

### What Can Be Simulated

**1. Unit Tests (pytest)**
```python
# test_auth_box.py
def test_vpn_detection():
    result = auth_box.check_vpn_connection()
    assert result.auth_type == AuthType.VPN
    assert isinstance(result.passed, bool)

def test_jira_client():
    client = JIRAClient("test_token")
    # Mock API response
    mock_data = {'status': 'Backlog'}
    assert client.parse_status(mock_data) == 'Backlog'
```

**2. Security Checks**
- âœ… Pre-commit hook blocks sensitive data
- âœ… .gitignore prevents staging
- âœ… Token patterns detected
- âœ… Customer names blocked

**Test Method:** 
```bash
# Simulate bad commit
echo 'TOKEN="ghp_fake123"' > test.py
git add test.py
./.git/hooks/pre-commit  # Should FAIL
```

**3. API Response Handling**
```python
# Mock JIRA API responses
def test_jira_status_fetch():
    with mock.patch('requests.get') as mock_get:
        mock_get.return_value.status_code = 200
        mock_get.return_value.json.return_value = {
            'fields': {'status': {'name': 'Backlog'}}
        }
        
        result = jira_client.get_issue_status('AAPRFE-999')
        assert result['status'] == 'Backlog'
```

**4. Data Parsing**
- âœ… Extract JIRA IDs from markdown
- âœ… Parse report templates
- âœ… Compare statuses
- âœ… Generate backups

**Test Method:** Use test data files, verify parsing logic

**5. Error Handling**
```python
def test_missing_token_error():
    with pytest.raises(AuthenticationError):
        auth_box.get_token(AuthType.JIRA_TOKEN, required=True)
```

---

## ğŸ“Š Testing Matrix

| Feature | Real Test | Simulated Test | Priority |
|---------|-----------|----------------|----------|
| GUI Launch | âœ… | âŒ | High |
| VPN Detection | âœ… | âœ… | High |
| Token Config | âœ… | âœ… | High |
| JIRA API Calls | âœ… | âœ… | High |
| Report Parsing | âŒ | âœ… | Medium |
| Error Messages | âœ… | âœ… | High |
| Pre-commit Hook | âœ… | âœ… | Critical |
| Report Updates | âœ… | âœ… | High |
| GitHub Issues | âœ… | âŒ | Medium |
| Cross-platform | âœ… | âŒ | Low |

---

## ğŸ”„ Test Workflow

### Phase 1: Development Testing (Continuous)

**While Building Features:**
1. Write unit tests first (TDD when appropriate)
2. Test locally with real data (CLI)
3. Verify with test data (`--test-data`)
4. Check GUI integration

```bash
# Example development test cycle
pytest tests/test_auth_box.py           # Unit tests
./tam-rfe check --test-data             # CLI real test
cd gui && npm start                      # GUI real test
```

### Phase 2: Feature Complete Testing

**Before Marking Feature "Done":**
1. âœ… Unit tests pass
2. âœ… Real user test (Jimmy)
3. âœ… Documentation updated
4. âœ… Error scenarios tested

### Phase 3: Release Testing

**Before Git Push:**
1. âœ… Run full test suite
2. âœ… Security checks pass
3. âœ… End-to-end workflow test
4. âœ… GUI smoke test

```bash
# Release testing checklist
pytest tests/                           # All unit tests
./.git/hooks/pre-commit                 # Security check
./tam-rfe check testcustomer            # Real workflow
cd gui && npm start                      # GUI verification
```

---

## ğŸ§ª Test Data Management

### Simulated Test Data (Safe for Git)

**Location:** `tests/fixtures/`

```
tests/
â””â”€â”€ fixtures/
    â”œâ”€â”€ sample_report.md          # Generic template
    â”œâ”€â”€ jira_response.json        # Mock API response
    â””â”€â”€ test_config.yaml          # Test configuration
```

**Contents:**
- Use AAPRFE-999, AAP-99999 (test IDs)
- Use "testcustomer" (generic name)
- Use fake tokens (ghp_fake123...)

### Real Test Data (Never Commit)

**Location:** `~/.taminator-data/test-data/`

```
~/.taminator-data/
â””â”€â”€ test-data/
    â”œâ”€â”€ testcustomer.md           # Generated at runtime
    â””â”€â”€ testcustomer2.md          # User-created
```

**Contents:**
- Created by `./tam-rfe check --test-data`
- Can use real JIRA API
- Uses real VPN/Kerberos
- Never committed to git

---

## ğŸ“ Test Documentation

### For Each Feature

**Required Documentation:**
1. **How to test manually**
   ```
   # Test tam-rfe check
   1. Ensure VPN connected
   2. Configure JIRA token
   3. Run: ./tam-rfe check --test-data
   4. Verify: Comparison table displays
   5. Expected: 5 issues, 4 up-to-date
   ```

2. **Automated tests**
   ```python
   # tests/test_check.py
   def test_check_command():
       # Arrange
       # Act
       # Assert
   ```

3. **Edge cases**
   ```
   - What if VPN disconnects mid-check?
   - What if JIRA API times out?
   - What if report file is corrupted?
   ```

---

## ğŸš¨ Security Testing (Critical)

### Always Test With Real Data First

**Why:** Security checks must work with actual sensitive data patterns

**Process:**
1. Create file with real token (in test branch)
2. Try to commit
3. Verify pre-commit hook blocks it
4. Delete file, reset branch

**Example:**
```bash
# Security test procedure
git checkout -b security-test

# Create bad file
echo 'TOKEN="actual_token_here"' > test.py
git add test.py

# Try to commit (should fail)
git commit -m "test"  # âŒ MUST BE REJECTED

# Clean up
git checkout main
git branch -D security-test
```

### Simulate for CI/CD

**For automated pipelines:**
```bash
# Create test file with patterns
echo 'ghp_1234567890abcdef' > test.tmp
git add test.tmp

# Run hook
./.git/hooks/pre-commit
# Expected exit code: 1 (failure)

# Cleanup
git restore --staged test.tmp
rm test.tmp
```

---

## ğŸ“ Testing Best Practices

### DO:
- âœ… Test happy path AND error cases
- âœ… Use real auth for integration tests
- âœ… Mock external APIs when appropriate
- âœ… Test security features with real patterns
- âœ… Verify error messages are helpful
- âœ… Test on clean environments (VM)

### DON'T:
- âŒ Skip real testing just because unit tests pass
- âŒ Commit sensitive data for "testing purposes"
- âŒ Test only happy path
- âŒ Assume GUI works because CLI works
- âŒ Skip cross-platform testing

---

## ğŸ“ˆ Testing Metrics

### Coverage Goals

**Unit Test Coverage:** 70%+ for core modules
- auth_box.py: 80%
- commands/*.py: 60%
- GUI: Manual testing only

**Integration Tests:** All critical workflows
- check â†’ update â†’ post pipeline
- Onboarding wizard
- Issue reporting

**Security Tests:** 100% of security features
- Pre-commit hook
- .gitignore patterns
- Token detection

---

## ğŸ”„ Continuous Testing

### During Development

```bash
# Watch mode for unit tests
pytest tests/ --watch

# Quick smoke test
./tam-rfe check --test-data

# GUI quick test
cd gui && npm start
```

### Before Each Commit

```bash
# Pre-commit checklist
pytest tests/                    # Unit tests pass
./.git/hooks/pre-commit          # Security check
git diff --cached | grep -i token  # Manual review
```

### Before Each Push

```bash
# Full validation
pytest tests/ --cov              # Coverage report
./tam-rfe check testcustomer     # Real workflow
python3 test_auth_audit.py       # Auth audit
cd gui && npm start              # GUI test
```

---

## ğŸ¯ Success Criteria

### Feature is "Done" When:

1. âœ… **Unit tests pass** - Automated tests green
2. âœ… **Real user test passes** - Jimmy verifies manually
3. âœ… **Error handling tested** - Edge cases covered
4. âœ… **Security checked** - No sensitive data leaked
5. âœ… **Documentation updated** - Test procedures documented
6. âœ… **Cross-platform verified** - Works on target platforms

---

*Testing is not just about finding bugs - it's about building confidence that Taminator will work reliably for all TAMs.*

